# StoryMuse Configuration
# Copy this file to .env and customize as needed

# LLM Server Configuration
# For Jan: http://localhost:1337/v1
# For Ollama: http://localhost:11434/v1
LLM_BASE_URL=http://localhost:1337/v1

# API Key (usually not needed for local LLMs)
LLM_API_KEY=not-needed

# Model to use (must be available on your LLM server)
LLM_MODEL=deepseek-r1-distill-qwen-7b
